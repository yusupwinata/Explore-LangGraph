{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c410e6c-6f3a-43a6-9cf5-f8ad0b23ea9d",
   "metadata": {},
   "source": [
    "# API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6f310d-03fe-4f60-8046-05b859e4a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f02a7-f1aa-4826-b0f6-6110d759fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}:\")\n",
    "\n",
    "# Input API Key manually\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be51712-f0db-4157-8bbc-91c57929ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "OLLAMA_API_KEY = \"ollama\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367fbf6-b5ba-4467-91e7-1645fc5e7a5e",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4909e7ae-0837-4cc2-ae61-1b21cf6fea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c892cb45-ee77-4c20-8cdf-d6e1c1739e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'),\n",
       " Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "[model for model in client.models.list().data][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60362d10-834a-4090-90f3-da840d211406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  ID              SIZE      MODIFIED    \n",
      "gemma3:latest         a2af6cc3eb7f    3.3 GB    3 weeks ago    \n",
      "deepseek-r1:latest    0a8c26691023    4.7 GB    3 weeks ago    \n",
      "llama3.2:latest       a80c4f17acd5    2.0 GB    3 weeks ago    \n",
      "deepseek-r1:1.5b      a42b25d8c10a    1.1 GB    3 weeks ago    \n",
      "tinyllama:latest      2644915ede35    637 MB    7 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812da2e1-d2cc-4f88-8042-a449331dad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llama_chat = ChatOpenAI(model=\"llama3.2:latest\", temperature=0, api_key=OLLAMA_API_KEY, base_url=OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18092f-8b2b-4a31-ab03-e6d6b4a4a2ab",
   "metadata": {},
   "source": [
    "# Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ff1ab-f465-47e7-bb5d-9fb338c2cbc6",
   "metadata": {},
   "source": [
    "## Faster Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d149e0-2ab4-42a3-8f5d-5b6e0d8ff9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 28, 'total_tokens': 51, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:latest', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-254', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--99c2fe57-bf8b-4b15-a70f-a6f8207cad1d-0', usage_metadata={'input_tokens': 28, 'output_tokens': 23, 'total_tokens': 51, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_chat.invoke(\"Hello There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daac046c-c8f3-4f20-a8ce-15c1250d73e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x000001CA0FC6F140>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_chat.stream(\"Hello There!\") # return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15a60a-4468-403e-a4ae-75538a0fe8fc",
   "metadata": {},
   "source": [
    "## Message List Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a620d50f-1199-466f-88ec-ed8cdfc5986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"It's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 28, 'total_tokens': 51, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2:latest', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-249', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--0ae18dfa-5993-4c25-be12-3bdff9ca7220-0' usage_metadata={'input_tokens': 28, 'output_tokens': 23, 'total_tokens': 51, 'input_token_details': {}, 'output_token_details': {}}\n",
      "\n",
      "It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "# Role: Human\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "msg = HumanMessage(content=\"Hello there!\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "# Invoke the model with message list\n",
    "response = llama_chat.invoke(messages)\n",
    "print(f\"{response}\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3ccf1-35f4-46c3-afb4-e67443ff2ddc",
   "metadata": {},
   "source": [
    "# Using Web Search Tools (module 4 sneak peek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d43aab-044a-442e-8a47-180b3d753ff1",
   "metadata": {},
   "source": [
    "## Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eefb63e-cb54-4683-920f-de85fb39fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b5ff5e4-ec56-4267-b3f5-6959d39daabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2afb1786-17e8-491b-9465-1c5333589a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "tavily_search = TavilySearchResults(max_results=2)\n",
    "search_docs = tavily_search.invoke(\"What is LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e42da6e-f57b-4020-a437-bcb43ce4eb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Large language model - Wikipedia',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Large_language_model',\n",
       "  'content': 'A **large language model** (**LLM**) is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") model designed for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\") tasks, especially [language generation](https://en.wikipedia.org/wiki/Natural_language_generation \"Natural language generation\"). LLMs are [language models](https://en.wikipedia.org/wiki/Language_model \"Language model\") with many [...] An LLM is a type of [foundation model](https://en.wikipedia.org/wiki/Foundation_model \"Foundation model\") (large X model) trained on language.[[35]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-35) LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.[[36]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-36)\\n\\n### Reinforcement learning from human feedback',\n",
       "  'score': 0.9288814},\n",
       " {'title': 'What is LLM? - Large Language Models Explained - AWS',\n",
       "  'url': 'https://aws.amazon.com/what-is/large-language-model/',\n",
       "  'content': 'Large language models, also known as LLMs, are very large [deep learning](https://aws.amazon.com/what-is/deep-learning/) models that are pre-trained on vast amounts of data. The underlying transformer is a set of [neural networks](https://aws.amazon.com/what-is/neural-network/) that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] *   [What is Cloud Computing?](https://aws.amazon.com/what-is-cloud-computing/ \"What is Cloud Computing?\")\\n*   [Cloud Computing Concepts Hub](https://aws.amazon.com/what-is/ \"Cloud Computing Concepts Hub\")\\n*   [Generative AI](https://aws.amazon.com/ai/generative-ai/)\\n\\nWhat is LLM (Large Language Model)?\\n===================================\\n\\n  \\n\\n[Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?pg=what_is_header) [...] [](https://aws.amazon.com/podcasts/ \"Podcast\")\\n\\n[](https://pages.awscloud.com/communication-preferences?trk=homepage \"Email\")\\n\\nAmazon is an Equal Opportunity Employer: _Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age._',\n",
       "  'score': 0.9037116}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5190e5b-7aad-403d-bf31-30198ac88294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Large language model - Wikipedia',\n",
       " 'url': 'https://en.wikipedia.org/wiki/Large_language_model',\n",
       " 'content': 'A **large language model** (**LLM**) is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") model designed for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\") tasks, especially [language generation](https://en.wikipedia.org/wiki/Natural_language_generation \"Natural language generation\"). LLMs are [language models](https://en.wikipedia.org/wiki/Language_model \"Language model\") with many [...] An LLM is a type of [foundation model](https://en.wikipedia.org/wiki/Foundation_model \"Foundation model\") (large X model) trained on language.[[35]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-35) LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.[[36]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-36)\\n\\n### Reinforcement learning from human feedback',\n",
       " 'score': 0.9288814}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f5cdd92-6c30-46f8-b64f-d6b838e60aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A **large language model** (**LLM**) is a [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") model designed for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing \"Natural language processing\") tasks, especially [language generation](https://en.wikipedia.org/wiki/Natural_language_generation \"Natural language generation\"). LLMs are [language models](https://en.wikipedia.org/wiki/Language_model \"Language model\") with many [...] An LLM is a type of [foundation model](https://en.wikipedia.org/wiki/Foundation_model \"Foundation model\") (large X model) trained on language.[[35]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-35) LLMs can be trained in different ways. In particular, GPT models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.[[36]](https://en.wikipedia.org/wiki/Large_language_model#cite_note-36)\n",
      "\n",
      "### Reinforcement learning from human feedback\n"
     ]
    }
   ],
   "source": [
    "print(search_docs[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f3212-0151-4ab2-801c-fb65ef44deaf",
   "metadata": {},
   "source": [
    "## Wikipedia (free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0427de7c-3c79-4968-8619-e3274f2506b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cd78016-43ff-4986-9750-0fcb25c6f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 2.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Page: Large language model\\nSummary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT or Gemini. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n\\n\\nPage: Claude (language model)\\nSummary: Claude is a family of large language models developed by Anthropic. The first model was released in March 2023.\\nThe Claude 3 family, released in March 2024, consists of three models: Haiku, optimized for speed; Sonnet, which balances capability and performance; and Opus, designed for complex reasoning tasks. These models can process both text and images, with Claude 3 Opus demonstrating enhanced capabilities in areas like mathematics, programming, and logical reasoning compared to previous versions. Claude 4, which includes Opus and Sonnet, was released in May 2025.\\n\\n\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search_wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "result = search_wikipedia.invoke(\"What is LLM?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8dc75-0776-4ad2-bd58-e35ca60ae8e3",
   "metadata": {},
   "source": [
    "# Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "090ecfb2-1041-4aae-b4ad-d25f8cbbae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5af26e2d-4c49-4b54-972e-03030a1c3653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Published: 2025-05-25\\nTitle: Interacting Large Language Model Agents. Interpretable Models and Social Learning\\nAuthors: Adit Jain, Vikram Krishnamurthy\\nSummary: This paper discusses the theory and algorithms for interacting large language\\nmodel agents (LLMAs) using methods from statistical signal processing and\\nmicroeconomics. While both fields are mature, their application to\\ndecision-making involving interacting LLMAs remains unexplored. Motivated by\\nBayesian sentiment analysis on online platforms, we construct interpretable\\nmodels and algorithms that enable LLMAs to interact and perform Bayesian\\ninference. Because interacting LLMAs learn from both prior decisions and\\nexternal inputs, they can exhibit bias and herding behavior. Thus, developing\\ninterpretable models and stochastic control algorithms is essential to\\nunderstand and mitigate these behaviors. This paper has three main results.\\nFirst, we show using Bayesian revealed preferences from microeconomics that an\\nindividual LLMA satisfies the necessary and sufficient conditions for\\nrationally inattentive (bounded rationality) Bayesian utility maximization and,\\ngiven an observation, the LLMA chooses an action that maximizes a regularized\\nutility. Second, we utilize Bayesian social learning to construct interpretable\\nmodels for LLMAs that interact sequentially with each other and the environment\\nwhile performing Bayesian inference. Our proposed models capture the herding\\nbehavior exhibited by interacting LLMAs. Third, we propose a stochastic control\\nframework to delay herding and improve state estimation accuracy under 2\\nsettings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives.\\nWe demonstrate the effectiveness of our methods on real datasets for hate\\nspeech classification and product quality assessment, using open-source models\\nlike LLaMA and closed-source models like ChatGPT. The main takeaway of this\\npaper, based on empirical analysis and mathematical formalism, is that LLMAs\\nact as rationally bounded Bayesian agents that exhibit social learning when\\ninteracting.\\n\\nPublished: 2025-02-28\\nTitle: LLM2: Let Large Language Models Harness System 2 Reasoning\\nAuthors: Cheng Yang, Chufan Shi, Siheng Li, Bo Shui, Yujiu Yang, Wai Lam\\nSummary: Large language models (LLMs) have exhibited impressive capabilities across a\\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\\nthese limitations are rooted in the foundational autoregressive architecture of\\nLLMs, which inherently lacks mechanisms for differentiating between desirable\\nand undesirable results. Drawing inspiration from the dual-process theory of\\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\\nresponsible for generating plausible candidates, while the verifier provides\\ntimely process-based feedback to distinguish desirable and undesirable outputs.\\nThe verifier is trained with a pairwise comparison loss on synthetic\\nprocess-supervision data generated through our token quality exploration\\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\\nself-consistency, LLM2 achieves additional improvements, boosting major@20\\naccuracy from 56.2 to 70.2 (+14.0).\\n\\nPublished: 2024-10-11\\nTitle: LLMD: A Large Language Model for Interpreting Longitudinal Medical Records\\nAuthors: Robert Porter, Adam Diehl, Benjamin Pastel, J. Henry Hinnefeld, Lawson Nerenberg, Pye Maung, Sebastien Kerbrat, Gillian Hanson, Troy Astorino, Stephen J. Tarsa\\nSummary: We introduce LLMD, a large language model designed to analyze a patient's\\nmedical history based on their medical records. Along with domain knowledge,\\nLLMD is trained on a large corpus of records collected over time and across\\nfacilities, as well as tasks and labels that make nuanced connections \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search_arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())\n",
    "result = search_arxiv.invoke(\"What is LLM?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06b22f4e-9dff-487e-88b4-73ced08ecee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf93411-d1bb-4e4a-8e5f-de631436fcff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugface",
   "language": "python",
   "name": "hugface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
